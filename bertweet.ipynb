{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMEn5T3hL2K5/0Ts04U0Jsw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniPioDelvecchio/GCNs_on_text/blob/issue-%235/bertweet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDfrQDhMf3fA",
        "outputId": "37cce261-2fa1-4e13-c632-06c478fa81ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-chFGaWUhYys"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnPke7Cvx1_v",
        "outputId": "eefd0dde-fbe2-45ed-88c4-2772cb19f20f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_frame = pd.read_csv(\"train_split.csv\")\n",
        "test_frame = pd.read_csv(\"test_split.csv\")\n",
        "val_frame = pd.read_csv(\"val_split.csv\")"
      ],
      "metadata": {
        "id": "f_KNQON5gEaI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tweets = list(train_frame.OriginalTweet.values)\n",
        "train_labels = list(train_frame.Sentiment.values)\n",
        "val_tweets = list(val_frame.OriginalTweet.values)\n",
        "val_labels = list(val_frame.Sentiment.values)\n",
        "test_tweets = list(test_frame.OriginalTweet.values)\n",
        "test_labels = list(test_frame.Sentiment.values)"
      ],
      "metadata": {
        "id": "zvh7KgeshaDC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
        "def encode_split(tweet_list, max_len=None):\n",
        "  pad = 'max_length' if max_len is not None else False\n",
        "  idxs = []\n",
        "  attn_masks = []\n",
        "  for t in tweet_list:\n",
        "    tok_seq = tokenizer.encode_plus(t,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_len,\n",
        "        padding=pad,\n",
        "        #pad_to_max_length=True,\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        #return_tensors='pt'\n",
        "\n",
        "    )\n",
        "    idxs.append(tok_seq.get('input_ids'))\n",
        "    attn_masks.append(tok_seq.get('attention_mask'))\n",
        "\n",
        "  return idxs, attn_masks\n",
        "\n",
        "tok_train, _ = encode_split(train_tweets)\n",
        "print(np.percentile([len(t) for t in tok_train], 90))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0khwX81pfP5r",
        "outputId": "8319c525-19d8-4c51-cde6-048f618b4487"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok_train, train_mask = encode_split(train_tweets, 60)\n",
        "tok_val, val_mask = encode_split(val_tweets, 60)\n",
        "tok_test, test_mask = encode_split(test_tweets, 60)"
      ],
      "metadata": {
        "id": "6FJWAbB2qatD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok_train = torch.tensor(tok_train)\n",
        "train_mask = torch.tensor(train_mask)\n",
        "tok_test = torch.tensor(tok_test)\n",
        "test_mask = torch.tensor(test_mask)\n",
        "tok_val = torch.tensor(tok_val)\n",
        "val_mask = torch.tensor(val_mask)"
      ],
      "metadata": {
        "id": "JlqVswYXsGMV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(tok_train, train_mask, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(tok_val, val_mask, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "c8gZzJxK0u-_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the BertClassfier class\n",
        "class FTBert(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self,bert, num_classes, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    num_classes: number of classes\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(FTBert, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        bert_hidden_size = bert.config.hidden_size\n",
        "        self.linear_1 = nn.Linear(bert_hidden_size, bert_hidden_size//2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear_2 = nn.Linear(bert_hidden_size//2, num_classes)\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = bert\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   out (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        out = outputs.last_hidden_state[:,0,:]\n",
        "        out = self.linear_1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear_2(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "iFSSs-aV1Udt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, scheduler, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch_i in range(epochs):\n",
        "    # =======================================\n",
        "    #               Training\n",
        "    # =======================================\n",
        "    # Print the header of the result table\n",
        "      print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9}\")\n",
        "      print(\"-\"*70)\n",
        "      # Reset tracking variables at the beginning of each epoch\n",
        "      total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "      # Put the model into the training mode\n",
        "      model.train()\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "        batch_counts +=1\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "        # Zero out any previously calculated gradients\n",
        "        model.zero_grad()\n",
        "        # Perform a forward pass. This will return logits.\n",
        "        logits = model(b_input_ids, b_attn_mask)\n",
        "        # Compute loss and accumulate the loss values\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        batch_loss += loss.item()\n",
        "        total_loss += loss.item()\n",
        "        # Perform a backward pass to calculate gradients\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        # Update parameters and the learning rate\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        # Print the loss values and time elapsed for every 20 batches\n",
        "        if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "          # Calculate time elapsed for 20 batches\n",
        "          # Print training results\n",
        "          print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9}\")\n",
        "          # Reset batch tracking variables\n",
        "          batch_loss, batch_counts = 0, 0\n",
        "          # Calculate the average loss over the entire training data\n",
        "          avg_train_loss = total_loss / len(train_dataloader)\n",
        "          print(\"-\"*70)\n",
        "\n",
        "      if evaluation == True:\n",
        "        # After the completion of each training epoch, measure the model's performance\n",
        "        # on our validation set.\n",
        "        val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "        print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f}\")\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    # Tracking variables\n",
        "    accuracies = []\n",
        "    losses = []\n",
        "    # For each batch in our validation set...\n",
        "    for batch in dataloader:\n",
        "      # Load batch to GPU\n",
        "      b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "      # Compute logits\n",
        "      with torch.no_grad():\n",
        "        logits = model(b_input_ids, b_attn_mask)\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        losses.append(loss.item())\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "        # Calculate the accuracy rate\n",
        "        acc = accuracy(preds, b_labels)\n",
        "        accuracies.append(acc)\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    loss = np.mean(losses)\n",
        "    acc = np.mean(accuracies)\n",
        "    return loss, acc\n",
        "\n",
        "def accuracy(pred_y, y):\n",
        "    \"\"\"Calculate accuracy.\"\"\"\n",
        "    return ((pred_y == y).sum() / len(y)).item()"
      ],
      "metadata": {
        "id": "zfHNvfZizDyv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hOJq5tCq0UaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(np.unique(train_labels)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvXYBL5H5o6F",
        "outputId": "a363dccd-3bc2-4c4d-e7de-6fdbbc39b63a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
        "bert_tweet.to(device)\n",
        "n_classes = len(np.unique(train_labels))\n",
        "\n",
        "epochs = 4\n",
        "optimizer = AdamW(bert_tweet.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "# Total number of training steps\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Set up the learning rate scheduler\n",
        "scheduler =get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0, # Default value\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "model = FTBert(bert_tweet, len(np.unique(train_labels)))\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "dzhua3Kl3uxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca9fb6a-3b2c-41b4-a83e-6eb8ca5aacfd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FTBert(\n",
              "  (linear_1): Linear(in_features=768, out_features=384, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (linear_2): Linear(in_features=384, out_features=5, bias=True)\n",
              "  (bert): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, optimizer, scheduler, train_dataloader, val_dataloader, 4, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erIkqKS6FOQe",
        "outputId": "2ad40f25-3727-4513-84dd-d4024e02daae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   1.584475   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   40    |   1.542093   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   60    |   1.463528   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   80    |   1.422577   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   100   |   1.373463   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   120   |   1.294061   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   140   |   1.292121   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   160   |   1.228018   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   180   |   1.196479   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   200   |   1.152804   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   220   |   1.111518   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   240   |   1.086524   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   260   |   1.141195   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   280   |   1.075081   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   300   |   1.066886   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   320   |   1.012164   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   340   |   1.023173   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   360   |   1.024562   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   380   |   1.031384   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   400   |   0.979478   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   420   |   0.963755   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   440   |   0.959549   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   460   |   0.988031   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   480   |   0.937994   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   500   |   0.949805   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   520   |   0.930563   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   540   |   0.990612   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   560   |   0.939579   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   580   |   0.902535   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   600   |   0.898032   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   620   |   0.914490   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   640   |   0.905950   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   660   |   0.891848   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   680   |   0.887627   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   700   |   0.919971   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   720   |   0.869097   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   740   |   0.889239   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   760   |   0.831944   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   780   |   0.916297   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |   797   |   0.853290   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   1.062482   |  0.819488  |   0.75   \n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.795804   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   40    |   0.826974   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   60    |   0.770250   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   80    |   0.811605   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   100   |   0.794240   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   120   |   0.815903   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   140   |   0.808579   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   160   |   0.806469   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   180   |   0.781773   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   200   |   0.768878   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   220   |   0.773874   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   240   |   0.770107   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   260   |   0.798473   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   280   |   0.793474   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   300   |   0.833694   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   320   |   0.748626   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   340   |   0.762588   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   360   |   0.765170   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   380   |   0.763371   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   400   |   0.728644   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   420   |   0.699311   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   440   |   0.709700   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   460   |   0.702503   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   480   |   0.710855   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   500   |   0.764081   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   520   |   0.710328   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   540   |   0.731679   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   560   |   0.735318   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   580   |   0.734844   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   600   |   0.763233   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   620   |   0.744259   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   640   |   0.756768   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   660   |   0.701353   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   680   |   0.699113   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   700   |   0.720453   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   720   |   0.703510   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   740   |   0.695463   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   760   |   0.661566   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   780   |   0.698104   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |   797   |   0.693431   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.751633   |  0.699440  |   0.80   \n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.632990   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   40    |   0.611335   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   60    |   0.617696   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   80    |   0.653593   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   100   |   0.624271   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   120   |   0.628441   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   140   |   0.647916   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   160   |   0.666556   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   180   |   0.637757   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   200   |   0.602837   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   220   |   0.626729   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   240   |   0.628965   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   260   |   0.600251   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   280   |   0.587334   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   300   |   0.623976   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   320   |   0.601394   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   340   |   0.582623   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   360   |   0.578305   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   380   |   0.597798   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   400   |   0.634162   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   420   |   0.602312   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   440   |   0.582190   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   460   |   0.629874   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   480   |   0.575624   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   500   |   0.590616   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   520   |   0.615099   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   540   |   0.640202   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   560   |   0.589574   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   580   |   0.643461   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   600   |   0.568193   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   620   |   0.584983   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   640   |   0.585406   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   660   |   0.598326   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   680   |   0.575284   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   700   |   0.605602   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   720   |   0.546314   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   740   |   0.557614   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   760   |   0.612124   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   780   |   0.605154   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |   797   |   0.603951   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.607466   |  0.645285  |   0.82   \n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.512113   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   40    |   0.595091   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   60    |   0.498722   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   80    |   0.545075   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   100   |   0.514330   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   120   |   0.536092   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   140   |   0.523793   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   160   |   0.509580   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   180   |   0.543300   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   200   |   0.523020   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   220   |   0.562603   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   240   |   0.524698   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   260   |   0.537016   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   280   |   0.529115   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   300   |   0.477885   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   320   |   0.494608   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   340   |   0.528825   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   360   |   0.523067   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   380   |   0.501997   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   400   |   0.538375   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   420   |   0.543674   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   440   |   0.510642   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   460   |   0.490602   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   480   |   0.511752   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   500   |   0.586609   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   520   |   0.532968   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   540   |   0.459459   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   560   |   0.510866   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   580   |   0.488924   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   600   |   0.486247   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   620   |   0.500339   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   640   |   0.518502   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   660   |   0.474827   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   680   |   0.503204   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   700   |   0.524907   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   720   |   0.461696   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   740   |   0.504952   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   760   |   0.507556   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   780   |   0.483905   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |   797   |   0.498076   |     -      |     -    \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.515537   |  0.636938  |   0.83   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DataLoader for our validation set\n",
        "test_labels = torch.tensor(test_labels)\n",
        "test_data = TensorDataset(tok_test, test_mask, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "test_loss, test_acc = evaluate(model, test_dataloader)\n",
        "print(f\"Test accuracy {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiy9O8k4Jbi3",
        "outputId": "80b8bffd-fcdc-4b43-e545-9640700e5a1d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-feabee9f39ff>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_labels = torch.tensor(test_labels)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy 0.819\n"
          ]
        }
      ]
    }
  ]
}